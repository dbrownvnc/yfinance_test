import streamlit as st
import yfinance as yf
import google.generativeai as genai
import pandas as pd
import os
import requests
import xml.etree.ElementTree as ET
import urllib.parse
from dateutil import parser
import re
import plotly.graph_objects as go
import time
import datetime
import socket
import concurrent.futures
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import math
import html
from difflib import SequenceMatcher

# ... (ì´ì „ ì½”ë“œì™€ ë™ì¼í•œ 1. ì„¤ì • ë° ì´ˆê¸°í™”, 2. ë°ì´í„° ê´€ë¦¬ í•¨ìˆ˜ ë¶€ë¶„) ...

# ---------------------------------------------------------
# 3. ê¸°íƒ€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ---------------------------------------------------------
# ... (get_robust_session, run_with_timeout, _fetch_history, _fetch_info, get_stock_name ë“± ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€) ...

def clean_html_text(text):
    if not text: return ""
    clean = re.sub(r'<[^>]+>', '', text)
    clean = html.unescape(clean)
    clean = " ".join(clean.split())
    return clean

def is_similar(a, b, threshold=0.7):
    if not a or not b: return False
    return SequenceMatcher(None, a, b).ratio() > threshold

def fetch_rss_realtime(url, limit=10):
    add_log(f"   ğŸŒ [RSS] Fetching URL: {url}")
    try:
        session = get_robust_session()
        response = session.get(url, timeout=5)
        root = ET.fromstring(response.content)
        items = []
        for item in root.findall('./channel/item')[:limit]:
            title = item.find('title').text
            link = item.find('link').text
            pubDate = item.find('pubDate').text
            description = ""
            desc_elem = item.find('description')
            if desc_elem is not None and desc_elem.text:
                description = clean_html_text(desc_elem.text)
            try: dt = parser.parse(pubDate); date_str = dt.strftime("%m-%d %H:%M")
            except: date_str = "ìµœì‹ "
            items.append({'title': title, 'link': link, 'date_str': date_str, 'summary': description})
        add_log(f"   âœ… [RSS] Parsed {len(items)} items.")
        return items
    except Exception as e:
        add_log(f"   âŒ [RSS] Error: {e}")
        return []

def get_realtime_news(ticker, name):
    """
    [ìˆ˜ì •ë¨] í‹°ì»¤ê°€ ì•„ë‹Œ 'ê³µì‹ ê¸°ì—…ëª…'ì„ ê¸°ì¤€ìœ¼ë¡œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    ê¸°ì¡´ì˜ Yahoo Finance(í‹°ì»¤ ê¸°ë°˜) ë¡œì§ì„ ê±´ë„ˆë›°ê³  Google News(ì´ë¦„ ê¸°ë°˜)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    add_log(f"ğŸ“° [NEWS] ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: {name} (Ticker: {ticker} ë¬´ì‹œ/ì°¸ê³ ìš©)")
    
    # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±: ë¬´ì¡°ê±´ ê¸°ì—…ëª…(name)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    # ì •í™•ë„ë¥¼ ìœ„í•´ ë”°ì˜´í‘œ("")ë¡œ ê°ì‹¸ì„œ êµ¬ë¬¸ ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤.
    search_query = f'"{name}"'
    
    add_log(f"   Trying Google News RSS with query: {search_query}")
    
    try:
        q_encoded = urllib.parse.quote(search_query)
        # hl=ko&gl=KR: í•œêµ­ì–´/í•œêµ­ ì„¤ì • (í•„ìš”ì‹œ en/USë¡œ ë³€ê²½ ê°€ëŠ¥í•˜ë‚˜, ì•± ì„¤ì •ìƒ ko ìœ ì§€)
        # ë§Œì•½ ì˜ë¬¸ ê¸°ì—…ì˜ ì˜ë¬¸ ë‰´ìŠ¤ë¥¼ ì›í•˜ì‹ ë‹¤ë©´ hl=en&gl=US ë“±ìœ¼ë¡œ ë³€ê²½ ê³ ë ¤ ê°€ëŠ¥
        url = f"https://news.google.com/rss/search?q={q_encoded}&hl=ko&gl=KR&ceid=KR:ko"
        
        google_news = fetch_rss_realtime(url, limit=7)
        for n in google_news: 
            n['source'] = "Google News"
            
        if google_news:
            return google_news
        else:
            add_log("   âš ï¸ Google News ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ.")
            return []
            
    except Exception as e:
        add_log(f"   âŒ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []

# ... (ì´í•˜ get_financial_metrics, sanitize_text ë° ë‚˜ë¨¸ì§€ UI/ë¡œì§ ì½”ë“œ ë™ì¼) ...
